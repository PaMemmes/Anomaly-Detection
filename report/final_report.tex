\documentclass[]{article}


\usepackage{makecell}
% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage{geometry}
\usepackage{minted}
\usepackage{amsfonts}
% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}

\DeclareMathOperator{\E}{\mathbb{E}}
\begin{document}
	
	\newgeometry{margin=2.5cm}
	\begin{titlepage}
		\thispagestyle{empty}
		\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
		\hspace{1cm}
		\center
		
		\textsc{\huge Ruprecht-Karls Universität Heidelberg}\\[2.0cm]
		
		\textsc{\Large Report}\\[1.0cm]
		
		\HRule\\[1.4cm]
		
		{ \huge \bfseries Anomaly-based detection of malicious network packages}\\[0.8cm]
		% { \large Untertitel der Arbeit, falls gewünscht}\\[0.3cm] % Falls nicht benötigt, einfach auskommentieren
		\HRule \\[8.4cm]
		
		
		\begin{minipage}[t]{0.8\textwidth}
			\begin{itemize}
				\item[\emph{Supervisor:}] Prof. Dr. Ullrich Köthe
				\item[\emph{Semester:}] SS 2022
				\item[\emph{Name:}] Pascal Memmesheimer
				\item[\emph{Matr.-Nr.:}] 3371798
			\end{itemize}
		\end{minipage}
		
		\vspace{2.5cm}
		
		\flushright \emph{Submission date:} \today
	\end{titlepage}
	\restoregeometry
	\pagenumbering{roman} % Start roman numbering
	
	\tableofcontents
	
	\newpage
	\pagenumbering{arabic} % Switch to normal numbers

	\section{Introduction}

	Cyber Security is an integral part of any system that is used in a productive environment and connected to some kind of network like the internet. It is absolutely critical to ensure the safety of a system in such unsafe environments. There are many ways to make a system safer. From securing the system by always downloading the newest security patches to using strong passwords or even not connecting a system to the internet at all. The latter is however often not possible. 
	\newline
	
	\noindent
	Firewalls have become an integral part of any system in a network. By using a firewall one can use rules to allow or deny network packets. This, however, is not sufficient in order to secure a system because the function of a firewall is not to detect attacks but to just define rules for network communication. Thus, inspecting incoming data over the network may be crucial to secure a system. This is done by a so-called Intrusion Detection System (IDS). An IDS can help identify malicious network packets from normal packets and based on this categorization rules can be established how to deal with these malicious network packets. An IDS deals with the sole detection of such malicious network packets, an Intrusion Prevention System (IPS) deals with its detection and prevention of such packets. 
	\newline
	
	\noindent
	IDSs can be classified into two different systems: Host-based intrusion detection systems (HIDS) and network intrusion detection systems (NIDS). A HIDS monitors and analyzes a running system by looking at the state of the system, the stored information, the filesystem, log files, and more. A NIDS monitors the network by analyzing the network traffic, extracting the relevant features of the network traffic, classifying it as unsafe or safe traffic, and then raising an alarm if unsafe traffic has been classified. In the following, we will focus solely on NIDSs.
	\newline
	
	\noindent
	There are mainly two different ways to implement a NIDS. Firstly, one can use a Signature-based intrusion detection system (SIDS). SIDSs primarily create a signature of a network package and then use pattern-matching techniques to find a known attack. This is also called knowledge-based or misuse detection \cite{10.1007/978-3-030-04503-6_14}. In practice, a database of intrusion signatures (i.e. malicious activities/attacks) is built and signatures of incoming traffic are calculated and then are compared with the existing database. This kind of detection of malicious network packages works excellently but only for attacks in which signatures of attacks are known beforehand and that are already stored in the database \cite{10.1145/972374.972384}. 
	
	SIDS methods have been used in commercial and existing systems like in Snort \cite{roesch1999snort} and NetSTAT \cite{vigna1999netstat}. Attacks that are not in the database or zero-day attacks (attacks that are previously unknown), however, cannot be detected at all. 
	
	Because of this anomaly-based intrusion detection systems (AIDS) have become increasingly popular, especially with the rise of Deep Learning and anomaly detection being an important part in many other different domains including fraud detection, medical imaging and now also Cyber Security - there, especially for intrusion detection. 
	\newline
	
	\noindent
	Anomaly detection is a well-studied problem although fast and effective methods on (usually) high-dimensional data remain a challenge and better solutions are sought after.  This is due to the fact that there are a lot of different anomaly types and it is not trivial to categorize them correctly. 
	
	Some network packets may only be an outlier when incorporating this outlier in the sequence of the network packets before and after that packet. This is then called a collective anomaly. Other times a network packet may be a normal packet but because of the specific context, it should be considered an anomaly, i.e. classified as an attack. This is called a contextual anomaly \cite{10.1145/1541880.1541882}. 
	\newline
	
	\noindent
	Another problem that poses itself is that a system needs to monitor network traffic in real time. This means that classification of network packages has to be relatively fast such that there is no significant and noticeable delay in the packets arriving. This will especially be of relevance of models that only implicitly model the data distribution which in turn means that in order to classify malicious network packets, a costly optimization problem has to be solved.
	\newline
	
	\noindent
	Lastly, it is relatively hard to obtain a large and well maintained dataset for this task partly because of the expensive labelling and also because many companies and institutions are afraid of accidentally releasing sensitive data. Also, often not many attack variants are covered by the dataset which leads to underperforming IDSs in real applications since only a small subset of attack variants are covered by the training data. 
	
	
	\section{Background and Theory}

	
	 The task is to implement a machine learning model that is able to differentiate between malicious packets vs normal packets, i.e. a binary classification task such that an IDS (or IPS) that uses this machine learning model can detect and reject malicious traffic. 
	 
	 This binary classification task is sometimes extended to multi-class classification. That way, an IDS is able to differentiate between different attacks. This can be useful because that way one could decide that less malicious attacks may not need to be rejected because that alarm may be a false positive and since it would not damage the system in a harmful way, it would not matter too much (for example a port scan is way less harmful than a DDoS).

	\subsection{Prior Work}
	 In the past, an abundance of different machine learning methods have been used for anomaly detection in various domains including for IDSs ranging from nearest neighbor methods \cite{5377998}, clustering approaches \cite{4244796}, decision trees \cite{6511281} to isolation forests \cite{DING201312} or support vector machines \cite{1234567}. 
	 \newline
	 
	 \noindent
	 Because of the rise of Deep Learning due to wider availability to computational resources and neural networks achieving excellent performance in many tasks, many approaches feature neural networks in some kind of way: Discriminative methods like feed forward networks, recurrent neural networks or LSTMs detect malicious packets (=anomalies) only if they are already labelled as such while methods based on autoencoders and/or variational autoencoders try to reconstruct normal data and then will identify data as anomalies when there is a high reconstruction error. This means, there generally is no need for labelled data in these generative models. 
	 
	 The threshold on how to differentiate between anomalies and normal data is often then adjusted to fit a specific use case, i.e. maximizing true positive rate while minimizing false positive rate or assuming reconstructions are normally distributed and labeling data as anomalies that are $x$ standard deviations away from the mean. This depends specifically on the business objective and the task at hand.
	 \newline
	 
	 \noindent
	 Other models that have been looked at are energy-based models and deep auto-encoding Gaussian mixture models that model the data distribution with autoencoders and then derive a statistical anomaly criterion based on energies or mixtures of Gaussians \cite{https://doi.org/10.48550/arxiv.1812.02288}. Bayesian networks and Hidden Markov Models have been used in the past too and achieve similar performance as the aforementioned methods \cite{inproceedings}.
	 \newline
	 
	 \noindent
	 The performance of these methods highly depend on the dataset used and the specific (hyper-parameter) being used, the methods have little systematic advantage over another when compared across datasets and (hyper-) parameters \cite{xxxx}, even classical machine learning models like the already mentioned k-nearest neighbor method and support vector machines are competitive. 
	 \newline
	 
	 \noindent
	 In the following we will look specifically at generative adversarial models and autoencoder, because we focused on the implementation of these models. 
	
	 
	 \subsection{Relevant tools}
	 \subsubsection{Generative Adversarial Nets}
	 Another model that has become popular in anomaly detection has been the generative adversarial network. It features a generator that tries to generate data that looks like the training data from a latent distribution (most often Gaussian or uniform) and a discriminator that tries to distinguish between data made by the generator and actual (real) data. The discriminator $D$ and the generator $G$ normally are determined by alternating gradient descent on the parameter and keeping the respective other model parameters fixed. This can be shown in listing \ref{lst}. It shows how the training procedure was done for our GAN model and how the discriminator's weights are fixed while training the generator. Generally, one could also use multiple steps per generator for example to make the generator be able to generate more realistic examples. 
	 \newline
	 
	 \noindent
	 When GAN have been introduced in the original paper the loss function being used was the Minimax loss, but this may be a suboptimal loss function and the usage of which specific loss function is still an active area of research and also depends strongly on the task. Many other losses have been proposed in the meantime. In the Minimax loss, the generator trains to minimize $log(1-D(G(z)))$ while the discriminator tries to maximize the probability of correctly distinguishing between fake (generated by Generator $G$) and real data, i.e. correctly assigning the right labels to data from the generator $G$ and real data \cite{https://doi.org/10.48550/arxiv.1406.2661}. 
	 
	 \begin{equation}
	 	\min_G \max_D V(D, G) = \E_{x\sim p_{data}(x)}[logD(x)] + \E_{z\sim p_z (z)}[log(1 - D(G(z)))] 
	 \end{equation}
	 
	 \noindent
	 This loss function can cause the GAN to get stuck in the early stages of training when the differentiation between fake and real samples done by the discriminator is very easy  \cite{https://doi.org/10.48550/arxiv.1406.2661}. A simple variation of this loss function is that the generator maximizes the logarithm of the discriminator probabilities which results in a more stable weight update mechanism. There are some other challenges with GAN loss functions:
	 
	 \begin{itemize}
	 	\item Mode Collapse: Generally, based on the random input that the generator is fed, one wants the GAN also to produce a wide variety of different outputs. Instead, the generator may learn to model a particular distribution of the data which gives a monotonous output or a really limited range of output.
	 	\item Vanishing Gradients: While vanishing gradients are most prominent when using activation functions like the sigmoid function, vanishing gradients can also occur in a GAN when the discriminator performs significantly better than the generator.
	 	\item Convergence: Because two networks are trained at the same time, it is really hard to stabilize both. When the generator gets better, the discriminator will become worse and it is hard to find the equillibrium between both.
	 \end{itemize}
	 

	\begin{lstlisting}[language=Python, caption=GAN Training, label={lst}]
def train_gan():
	for epochs:
		for batches:
			noise = generate_noise()
			generated_fakes = generator.generate(noise)
	
			discriminator.trainable = True
			discriminator_loss = discriminator.train(X_normal, generated_fakes)
	
			noise = generate_noise()
			discriminator.trainable = False
			generator_loss = gan.train(noise, np.ones())
	\end{lstlisting}
	
	\noindent
	Because GANs are generative models, one cannot use them in the "classical" sense for anomaly detection. One can instead exploit the GAN architecture and its strong classifying abilities or its strong generative abilities, and for that, multiple approaches have been developed. For a data point one could "invert" the generator in order to find latent variables that minimize the reconstruction error \cite{https://doi.org/10.48550/arxiv.1703.05921}. though this is computationally expensive since each gradient computation requires backpropagation through the generator $G$ and this is suboptimal since classification of malicious traffic needs to be done in real-time as already mentioned. 
 	  
 	 \subsubsection{Autoencoder}
 	  
 	  Autoencoders are neural networks that are trained to reconstruct its inputs. They are a form of unsupervised learning that learn efficient encodings of the data that they are fed but can also be used in clustering tasks and as such can be used for anomaly detection. 
 	  \newline
 	  
 	  \noindent
 	  Autoencoders consists of two parts: An encoder and a decoder. They can be viewed as functions $z = f(x)$ and $r = g(z)$ respectively, where $x$ is the input data. The encoder hereby tries to compress the (usually) high-dimensional data into a low-dimensional representation and the decoder aims to reconstruct the originally inputted data from the compressed data representation generated by the encoder. The goal of training the encoder is to minimize the difference between output and input by finding the optimal parameters $\theta$. This can be done by the MSE loss, i.e. 
 	  
 	  \begin{equation}
 	  	L(x, y) =  \frac{1}{N} \sum_{i=1}^{N} ||x_i - y_i ||^2
 	  \end{equation}
 	  where $N$ is the total number of instances in the training set. The optimal parameters $\theta$ are then found by:
 	  
 	  \begin{equation}
 	  	\theta = \operatorname*{argmin}_\theta
 	  	 L(x,y)
 	  \end{equation}
	 \section{Method}
	 
	 \subsection{Preprocessing}
	 The dataset that we used was the CSE-CIC-IDS2017 (CICIDS2017) dataset. There actually is a new updated dataset, the CSE-CIC-IDS2018 (CICIDS2018) dataset, but this dataset was too large for our computing ressources. The CICIDS2018 dataset has a similar ratio of malicious to normal packets compared to the CICIDS2017 dataset but contains 16,233,002 instances while the CICIDS2017 "only" has a total of 2,830,743 instances. 
	 
	 Each packet in the CICIDS2017 dataset has a total number of 78 different features and a label which ranged from "BENIGN" to any of the different attacks which were a total of 14 different attack types. None of the features were categorical (except the label of course). 
	 \newline
	 
	 
	 \noindent
	 In preprocessing, we removed any row that had some kind of faulty entry in the features, i.e. removing any rows that had infinity/-infinity in it or if there was a NaN value. After dropping these values, the dataset still had $99.89\%$ of its original size. We then made a 75\%-25\% train-test split and saved this configuration such that it could be used for any of the models that we implemented. 
	 
	 We also scaled the data using the MinMaxScaler that scales the input according to the following formula: $x_{scaled} = \frac{x-min(x)}{max(x)-min(x)}$ which is part of scikitlearns library \cite{sklearn_api}. We then changed the labels of the data to be binary. That way, attacks can be detected but the models are not able to predict the specific type of attack. This means that the models should be able to differentiate between normal traffic and malicious traffic (which consisted previously out of 14 different types: DDoS, Infiltration, Heartbleed, PortScan, etc.). We had to simplify the task to binary predictions because the approach for the GAN would not have worked otherwise and this binary classification challenge on the CICIDS (2017 and 2018) dataset seem to be the most popular. 

	 \subsection{Approach}

	 Our main contributions are a GAN and an autoencoder model and we compare their performance for anomaly detection on the CICIDS2017 dataset. We also implemented a simple Feed Forward Network which served as a baseline model and implemented also a Logistic Regression model to have some kind of comparison from the neural models to some non-neural model. In the following we will describe and explain each of the neural models.

	 
		
	  \subsubsection{Feed forward network}
	 The first model that we implemented is our baseline model which consists of a simple feed-forward network making binary predictions on the data using the binary cross entropy loss function. The baseline network had the problem of always predicting that the traffic is normal and still getting an accuracy of $80\%$. In order deal with that fact, we decided to adjust the class weights for the two classes. We had to adjust it in a really biased way towards the anomaly class, i.e. assigning a weight of $0.95$ to the anomaly class and $0.05$ to the normal class. 

	 
	 
	\subsubsection{Generative Adversarial Network}
	 Our main contribution was to develop a GAN that is able to differentiate between malicious and normal packets. Since GANs normally are generative models that are used to generate data (most often images) the GAN has to be modified in some way to be able to make it a discriminative model.  The idea is that the trained discriminator during training becomes really good at classifiying real data (i.e. the distribution of the real data) and fake data (i.e. data generated by the generator), i.e. it can differentiate between the distribution of the real data and any other data.
	 \newline
	 
	 \noindent
	 There are multiple ways the discriminator is then able to identify anomalies, for example by incorporating an autoencoder architecture into the GAN architecture and then using the reconstruction scores to be able to classify data. The approach we used, however, is by using the following assumption: Because the training (and test) data comprises of approximately 20\% of anomalies, ideally the lowest 20\% scores should constitute anomalies. That way, we can use a simple threshold based on the score outputted by the discriminator. 
	 
	 
	 The prediction of the network is done way faster than trying to recover the latent representation of a given input example by solving an optimization problem and thus can be used in real-time applications like an IDS.
	 \newline
	 
	 \noindent
	 This can be done by simply training the GAN on a subset of the training data, i.e. by training the GAN with only normal (=not malicious) network packets such that the discriminator gets proficient at being able to classify normal and fake (from the generator) packets. This way, the discriminator may be able to then differentiate between normal and malicious traffic. 
	 
     \subsubsection{Autoencoder}
     As already explained, the autoencoder consists of an encoder and a decoder. We used a latent dimension of 4 and used the Mean Squared Error loss. In order to make predictions, we generate predictions (i.e. reconstructions) using the autoencoder model of the test dataset. Then we compute the reconstruction scores by using the mean squared error between the reconstructed test data and the test data, i.e. $recon\_scores = \frac{1}{N} \sum_{i=1}^{N} (Y_i - \hat{Y}_i)^2$, where $N$ is the number of test data points, $Y_i$ is the true test data point and $\hat{Y}_i$ is the reconstructed test data point. Based on these reconstruction scores a threshold can be set that accomplishes some kind of objective.
     

	\subsubsection{Thresholds}
     When making binary predictions it is important to decide how to model the decision component, i.e. after creating the model that outputs probabilities $(\hat{p}, 1-\hat{p})$, how to decide at which probability threshold the decision should be made to one class or to the other. The decision on how to find the optimal decision threshold varies greatly depending on the task at hand: In medical applications a false negative could potentially not diagnose a disease which could lead to the patient not getting the right treatment.
     \newline
     
     \noindent
     For anomaly detection it is not as obvious to decide what one should try to minimize. Is it better to minimize the detection of false positives or false negatives? The former leads to legit network traffic being rejected, the latter leading to attacks that are not being detected. For the feed forward network and the autoencoder we maximized the true positive ratio and minimizing the false positive ratio. As already hinted at above, for the GAN we used the lowest $x\%$ scores where $x$ is the number of anomalies in the training data set ($\approx20$\%).
     
	
	
	\section{Experiments}
	First and foremost, we noticed that multiple executions of the code resulted in vastly different results even when initializing the weights of the network with a seed for every neural network that was implemented. 
	\newline
	
	\noindent
	We evaluated the model based on different metrics. We measured the (validation) accuracy as well as measuring recall and precision and f1-score, which is the harmonic mean of precision and recall, according to the following equations, where TP denotes true positive, TN denotes true negatives, FP denotes false positives and FN denotes false negatives: 
	
	\begin{equation}
		Precision = \frac{TP}{TP + FP}
	\end{equation}
	
	\begin{equation}
		Recall = \frac{TP}{TP + FN}
	\end{equation}
	\begin{equation}
		Accuracy = \frac{TP + FN}{TP+FP+TN+FN}
	\end{equation}
\begin{equation}
	F1 = \frac{TP}{TP + \frac{1}{2} (FP+FN)}
\end{equation}
	We also calculated the ROC curve and made a confusion matrix. Since there is a class imbalance of anomalous and normal packets, accuracy is not a good performance indicator. Simply having a classifier that always predicts normal packets would yield an accuracy score of 80\% which in itself is not a particularly bad score but fails to solve the task. That's why we also have a look at the aforementioned metrics. 
	\newline
	
	\begin{table}[]
		\begin{tabular}{llll}
			& \textbf{FFN}        & \textbf{Autoencoder} & \textbf{GAN}        \\
			\textbf{Learning rate} & 0.001               & 0.000001             & 0.00001             \\
			\textbf{Optimizer}     & Adam                & Adam                 & Adam                \\
			\textbf{Batch size}    & 512                 & 512                  & 512                 \\
			\textbf{Epochs}        & 50                  & 50                   & 10                  \\
			\textbf{Loss function} & Binary Crossentropy & Binary Crossentropy  & Binary Crossentropy \\
			\textbf{Trainable Parameters} & 46,657 &  35,602 & 43,919 \\
		\end{tabular}
		\caption{Parameters for models}
		\label{para}
	\end{table}

	\noindent
	In table \ref{para} the configuration of each neural network can be found. This was the best configuration found after experimenting with the hyperparameters. Using more epochs would likely increase the performance noticeably but we did not have enough computational resources to increase the epochs. Also, we had to differ in using the learning rate because otherwise the autoencoder and the GAN would update its gradients such that it would jump over minima and would yield worse performance. 
	
	We also noticed that based on the generated random noise that was fed into the generator the performance of the GAN (specifically the performance of the discriminator when trying to classify anomalies) varied greatly. 
	\newline
	
	\noindent
	In addition to that, the GAN and the autoencoder failed to converge to an acceptable performance. Even though the losses of the autoencoder and the GAN are decreasing relatively steadily and seem like they converge as can be seen in Figure \ref{loss}, they both have bad performance.
	\newline
	
		\begin{figure}[!tbp]
		\centering
		\subfloat[Feed forward network]{\includegraphics[width=0.3\textwidth]{best/baseline/loss_baseline.png}\label{fig:f4}}
		\hfill
		\subfloat[Autoencoder]{\includegraphics[width=0.3\textwidth]{best/autoencoder/loss_autoencoder.png}\label{fig:f5}}
		\hfill
		\subfloat[GAN]{\includegraphics[width=0.3\textwidth]{best/gan/loss_gan.png}\label{fig:f6}}
		\caption{Loss training}
		\label{loss}
	\end{figure}

	\begin{figure}[!tbp]
	\centering
	\subfloat[Feed forward network]{\includegraphics[width=0.4\textwidth]{best/baseline/accuracy_baseline.png}\label{fig:f1}}
	\hfill
	\subfloat[Autoencoder]{\includegraphics[width=0.4\textwidth]{best/autoencoder/accuracy_autoencoder.png}\label{fig:f2}}
	\caption{Accuracy in training}
	\label{accuracy}
	\end{figure}

	\noindent
	In Figure \ref{accuracy} the training and validation accuracy can be seen over the epochs trained from the autoencoder and the feed forward network. While the feed forward network already has a high accuracy to begin with, the autoencoder struggles to get the same accuracy as the feed forward network in its very first iteration. Both of their validation accuracy exceeds the training accuracy because of regularization (dropout) which decreases its training accuracy because not the full potential of the network is being used. 
	\newline
	
			\begin{table}[]
		\begin{tabular}{lllll}
			& \textbf{Logistic Regression} & \textbf{FFN} & \textbf{Autoencoder} & \textbf{GAN} \\
			\textbf{Accuracy}  & 92.4                       & 0.001        & 50.5             & 79.3      \\
			\textbf{Recall}    & 77.0                         & Adam         & 99.8                 & 47.7        \\
			\textbf{Precision} & 83.6                         & 512          & 28.5                  & 47.7         \\
			\textbf{F1-score}  & 80.2                         & 50           & 44.3                   & 47.7        
		\end{tabular}
		\caption{Metrics of models}
		\label{table1}
	\end{table}

	\noindent
	In table \ref{table1} the result of the 4 different models can be seen. The feed forward network has achieved the best metrics in every single category surpassing the other models greatly. It has a significantly higher recall than precision which can be desirable since a high recall shows that it almost recognized every bit of the malicious traffic. The lower precision shows that the network did classify some normal traffic as malicious traffic which basically suggests that the network is careful to not classify anomalous traffic as normal one and rather misclassifies normal traffic as malicious traffic than to misclassify malicious traffic as normal one.
	\begin{figure}[!tbp]
		\centering
		\subfloat[Feed forward network]{\includegraphics[width=0.3\textwidth]{best/baseline/roc_baseline.png}\label{fig:f7}}
		\hfill
		\subfloat[Autoencoder]{\includegraphics[width=0.3\textwidth]{best/autoencoder/roc_autoencoder.png}\label{fig:f8}}
		\hfill
		\subfloat[GAN]{\includegraphics[width=0.3\textwidth]{best/gan/roc_gan.png}\label{fig:f9}}
		\caption{ROC curves}
		\label{roc}
	\end{figure}

	This is due to the fact that we adjusted the class weights heavily in favor of the anomalies. We used class weights of 0.05 for normal packets and 0.95 for anomalous traffic since the class imbalance is skewed towards normal packets and we want to ensure that anomalous traffic is almost 100\% correctly captured by the model. This can also be seen in Figure \ref{confusion} where there are only 988 instances of malicious traffic wrongly classified as normal traffic. In contrast to that, the network actually has a lot of false positives with 13,222 normal packets classified as anomalies.
	\newline
	
	\noindent
	It is important to note however, that the performance of the Logistic Regression model may not be competitive to that of the feed forward network, but it sill has a really good performance overall, especially considering that we did not any special preprocessing beforehand and implemented it quickly just as a comparison. When tuning parameters and preprocessing the data properly for the Logistic Regression model, we suppose that the performance in every aspect could increase significantly.
	\newline
	

	\begin{figure}[!tbp]
	\centering
	\subfloat[Feed forward network]{\includegraphics[width=0.3\textwidth]{best/baseline/confusion_baseline.png}\label{fig:f7}}
	\hfill
	\subfloat[Autoencoder]{\includegraphics[width=0.3\textwidth]{best/autoencoder/confusion_autoencoder.png}\label{fig:f8}}
	\hfill
	\subfloat[GAN]{\includegraphics[width=0.3\textwidth]{best/gan/confusion_gan.png}\label{fig:f9}}
	\caption{Confusion matrices}
	\label{confusion}
\end{figure}

	\section{Summary and Outlook}
	The results did not look like we expected them to look like. We thought that the autoencoder and the GAN surpass the performance of the Feed Forward Network significantly, but this did not happen because we could not make the autoencoder or the GAN converge to a good minimum. We expected the autoencoder and the GAN to have a similar performance to each other. 
	
	We tested the GAN and autoencoder on another dataset and our assumption was confirmed. On this dataset they both had an AUC (Area Under Curve) of 1.0 and achieved a high overall accuracy with a similar performance. 
	
	\subsection{Outlook}
	\noindent
	One could look into Bayesian Hyper-Parameter Optimization with Gaussian Processes in order to make the GAN converge to a good optimum more likely. This is especially important because often GANs are hard to train and to make them converge. Training the GAN for this task is especially hard, because there are no good objective metrics for evaluating whether a GAN is performing well during training, so one has to visually inspect the generated examples by the generator \cite{https://doi.org/10.48550/arxiv.1606.03498}. This cannot be done in this specific task because the generated examples are not images but a 78 dimensional vector of different values (i.e. packet length, etc.). This makes it hard to ensure that the training is actually going well. 
	\newline
	
	\noindent
	Furthermore, there are many extensions for GANs that could be viable for usage in an IDS. As we already have talked about using an Adversarial Autoencoder or using Adversarial Variational Bayes could be extensions and may be more powerful than using a simple GAN. Also, we thought about implementing a convolutional GAN but did not do that since the simple GAN was not working properly to begin with. Also we are not sure if the advantages a CNN has on image data translates to the data in this cybersecurity context. 
	\newpage
	\bibliographystyle{plain}
	\bibliography{references}
\end{document}

